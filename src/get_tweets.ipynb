{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "This file can by used for scraping tweets by using Twitter API. To run this file, firstly you have to create your account on https://apps.twitter.com/. After doing it, create .env file in the main directory of the project, where you save your private API credentials - CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_SECRET.\n",
    "\n",
    "WARNING:\n",
    "You can get tweets from maximum last 7 days!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell just once! (or restart Kernel before second time)\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir('..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from utils.fixed import ENV_PATH, MAIN_PATH, DATA_PATH, load_match_data, MATCH_DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_short_df(tweets, hashtag_list, teams):\n",
    "    # load tweets to small dataframe having only 5 columns obligatory and enough for this project\n",
    "    DataSet = pd.DataFrame()\n",
    "    DataSet['tweetID'] = [tweet.id for tweet in tweets]\n",
    "    DataSet['tweetText'] = [tweet.text for tweet in tweets]\n",
    "    DataSet['tweetCreated'] = [tweet.created_at for tweet in tweets]\n",
    "    DataSet['hashtag'] = [hashtag for hashtag in hashtag_list]\n",
    "    DataSet['team'] = [team for team in teams]\n",
    "    return DataSet\n",
    "\n",
    "def to_long_df(tweets, hashtag_list, teams):\n",
    "    # load tweets to bigger dataframe having many additional data which can be used in further analyses\n",
    "    DataSet = pd.DataFrame()\n",
    "    DataSet['tweetID'] = [tweet.id for tweet in tweets]\n",
    "    DataSet['tweetText'] = [tweet.text for tweet in tweets]\n",
    "    DataSet['tweetCreated'] = [tweet.created_at for tweet in tweets]\n",
    "    DataSet['tweetRetweetCt'] = [tweet.retweet_count for tweet in tweets]\n",
    "    DataSet['tweetFavoriteCt'] = [tweet.favorite_count for tweet in tweets]\n",
    "    DataSet['tweetSource'] = [tweet.source for tweet in tweets]\n",
    "    DataSet['userID'] = [tweet.user.id for tweet in tweets]\n",
    "    DataSet['userScreen'] = [tweet.user.screen_name for tweet in tweets]\n",
    "    DataSet['userName'] = [tweet.user.name for tweet in tweets]\n",
    "    DataSet['userCreateDt'] = [tweet.user.created_at for tweet in tweets]\n",
    "    DataSet['userDesc'] = [tweet.user.description for tweet in tweets]\n",
    "    DataSet['userFollowerCt'] = [tweet.user.followers_count for tweet in tweets]\n",
    "    DataSet['userFriendsCt'] = [tweet.user.friends_count for tweet in tweets]\n",
    "    DataSet['userLocation'] = [tweet.user.location for tweet in tweets]\n",
    "    DataSet['userTimezone'] = [tweet.user.time_zone for tweet in tweets]\n",
    "    DataSet['hashtag'] = [hashtag for hashtag in hashtag_list]\n",
    "    DataSet['team'] = [team for team in teams]\n",
    "    return DataSet\n",
    "\n",
    "def get_tweets(team, hashtag, day, month, year, hour, mins):\n",
    "    # get tweets for a hashtag from specified period of time (starting from match_start date)\n",
    "    startDate = datetime.datetime(year, month, day, hour, mins, 0)\n",
    "    endDate = startDate + datetime.timedelta(minutes=150)\n",
    "    startDate_param = datetime.datetime.strftime(startDate, '%Y-%m-%d')\n",
    "    endDate_param = datetime.datetime.strftime(startDate + datetime.timedelta(days=1), '%Y-%m-%d')\n",
    "    tmpTweets = api.search(q=hashtag, lang=\"en\", since=startDate_param, until=endDate_param, count=100)\n",
    "    tweets = []\n",
    "    hashtag_list = []\n",
    "    teams = []\n",
    "    try:\n",
    "        while (tmpTweets[-1].created_at > startDate):\n",
    "            tmpTweets = api.search(q=hashtag, lang=\"en\", max_id=tmpTweets[-1].id, count=100)\n",
    "            for tweet in tmpTweets:\n",
    "                if tweet.created_at < endDate and tweet.created_at > startDate:\n",
    "                    tweets.append(tweet)\n",
    "                    hashtag_list.append(hashtag)\n",
    "                    teams.append(team)\n",
    "    except:\n",
    "        print(\"No tweets for hashtag: \" + hashtag)\n",
    "    return tweets, hashtag_list, teams\n",
    "\n",
    "def tweets_to_csv(hashtag_dict, date, file_name):\n",
    "    # save tweets to csv file\n",
    "    tweets = []\n",
    "    hashtags = []\n",
    "    teams = []\n",
    "    for key, value in hashtag_dict.items():\n",
    "        for hashtag in value:\n",
    "            print('starting ', hashtag)\n",
    "            tweets_temp, hashtag_list_temp, teams_temp = get_tweets(key, hashtag, date.day, date.month, date.year,\n",
    "                                                                    date.hour, date.minute)\n",
    "            tweets += tweets_temp\n",
    "            hashtags += hashtag_list_temp\n",
    "            teams += teams_temp\n",
    "    df = to_short_df(tweets, hashtags, teams)\n",
    "    df.to_csv(file_name, sep=';', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD TWITTER API CREDENTIALS FROM .ENV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path=ENV_PATH)\n",
    "\n",
    "consumer_key = os.getenv(\"CONSUMER_KEY\")\n",
    "consumer_secret = os.getenv(\"CONSUMER_SECRET\")\n",
    "access_key = os.getenv(\"ACCESS_KEY\")\n",
    "access_secret = os.getenv(\"ACCESS_SECRET\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONNECT WITH TWITTER API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINE THE MATCH ON WHICH YOU WANT TO SCRAP TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATCH = \"SOU-CHE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD MATCH DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name, team1, team2, match_start, first_part_end, second_part_start, \\\n",
    "match_end, hashtags_team1, hashtags_team2 = load_match_data(MATCH)\n",
    "match_hashtags = {team1: hashtags_team1, team2: hashtags_team2}\n",
    "scrapping_start_date = datetime.datetime.strptime(match_start, \"%Y-%m-%d %H:%M:%S\") - datetime.timedelta(minutes=30)\n",
    "TWEETS_CSV_PATH = os.path.join(MAIN_PATH, DATA_PATH, file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEFORE YOU RUN TWEETS_TO_CSV FUNCTION,  CHECK IF THE DATA IS CORRECT LOADED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SOU_CHE.csv',\n",
       " 'Southampton',\n",
       " 'Chelsea',\n",
       " '2018-10-07 13:15:00',\n",
       " '2018-10-07 14:02:00',\n",
       " '2018-10-07 14:17:00',\n",
       " '2018-10-07 15:06:00',\n",
       " ['#saintsfc', '#southamptonfc', '#wemarchon'],\n",
       " ['#chelseafc',\n",
       "  '#chelsea',\n",
       "  '#cfc',\n",
       "  '#cfcfamily',\n",
       "  '#cfcfans',\n",
       "  '#chelseafans',\n",
       "  '#coyb',\n",
       "  '#comeonyoublues',\n",
       "  '#theblues',\n",
       "  '#blueisthecolour'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name, team1, team2, match_start, first_part_end, second_part_start, match_end, hashtags_team1, hashtags_team2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET TWEETS AND SAVE TO CSV\n",
    "##### WARNING: With basic twitter-api-account you can only get tweets maximum one week old !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  #saintsfc\n",
      "starting  #southamptonfc\n",
      "starting  #wemarchon\n",
      "starting  #chelseafc\n",
      "starting  #chelsea\n",
      "starting  #cfc\n",
      "starting  #cfcfamily\n",
      "starting  #cfcfans\n",
      "starting  #chelseafans\n",
      "starting  #coyb\n",
      "starting  #comeonyoublues\n",
      "starting  #theblues\n",
      "starting  #blueisthecolour\n"
     ]
    }
   ],
   "source": [
    "tweets_to_csv(match_hashtags, scrapping_start_date, TWEETS_CSV_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
